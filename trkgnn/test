# training output directory
output_dir: ./output
# random number seed
rndm: 1

data:
  #--------------------------------
  # There are two ways to read data:
  # 1. Read from ROOT (default)
  # 2. Read from graph
  #    -- Graph is built from script: `extra_script/graph_to_disk.py`
  #    -- One Graph file is a list of graphs to be one chunk
  #--------------------------------
  # --> Read from ROOT (disable if read from graph)
  # input directory for data
  input_dir: "/Users/avencast/CLionProjects/darkshine-simulation/workspace/Tracker_GNN*.root"
  tree_name: 'dp'
  # how many total events to train
  global_stop: 1000000
  # how many events loaded per training period (Not total)
  chunk_size: "50 MB"

  # --> Read from graph
  read_from_graph: true
  # input directory for graph
  input_graph_dir: "/Users/avencast/CLionProjects/darkshine-simulation/workspace/test_output/"
  # how many files to read per training period (Not total) 
  # (set to -1 to read all files)
  global_stop_graph_file: -1

  # --> General Data Loader Settings
  # which collection to use
  collection: 'TagTrk1'
  batch_size: 1
  # number of works using CPU (0 disable multi-threading)
  n_workers: 0
  # initial incident energy (normalization constant)
  E0: 2900
  # move large graph to only validation stage
  min_graph_size: 1 # MB
  
  # This is mainly for application stage (automatically true)
  #   -- it's more or less determined in the training stage and graph building stage
  #   -- If input graph or trained model has 3 features, then it's trained without BField
  #   -- If input graph or trained model has 6 features, then it's trained with BField
  graph_with_BField: true
  

# make sure to choose the correct model if to predict momentum
momentum_predict: true
model:
  # model to predict momentum
  name: mpnn_p
  # data input dimension (x,y,z) = 3
  input_dim: 3
  # network settings
  n_edge_layers: 4
  n_node_layers: 4
  hidden_dim: 64
  n_graph_iters: 8
  layer_norm: true

# loss function
loss_func: binary_cross_entropy_with_logits

optimizer:
  name: Adam
  learning_rate: 0.001
  weight_decay: 1.e-5
  # decay shcedule for optimizer
  lr_decay_schedule:
    - { start_epoch: 36, end_epoch: 64, factor: 0.1 }
    - { start_epoch: 64, end_epoch: 82, factor: 0.01 }
    - { start_epoch: 82, end_epoch: 100, factor: 0.01 }

training:
  # training epochs
  n_epochs: 90
  # total training epochs
  n_total_epochs: 100
